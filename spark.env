# Spark master
# https://spark.apache.org/docs/3.3.1/spark-standalone.html

# Bind the master to a specific hostname or IP address, for example a public one
SPARK_MASTER_HOST=spark-master
# Start the master on a different port (default: 7077)
SPARK_MASTER_PORT=7077
# Port for the master web UI (default: 8080)
SPARK_MASTER_WEBUI_PORT=8080
# Configuration properties that apply only to the master in the form "-Dx=y" (default: none).
# SPARK_MASTER_OPTS



# Spark worker
# https://spark.apache.org/docs/3.3.1/spark-standalone.html

# Total number of cores to allow Spark applications to use on the machine (default: all available cores).
# SPARK_WORKER_CORES=32
# Total amount of memory to allow Spark applications to use on the machine, e.g. 1000m, 2g (default: total memory minus 1 GiB); 
# note that each application's individual memory is configured using its spark.executor.memory property.
# SPARK_WORKER_MEMORY=64g
# Start the Spark worker on a specific port (default: random).
# SPARK_WORKER_PORT
# Port for the worker web UI (default: 8081)
SPARK_WORKER_WEBUI_PORT=8081
# Configuration properties that apply only to the worker in the form "-Dx=y" (default: none). 
# SPARK_WORKER_OPTS



# Spark demon
# https://spark.apache.org/docs/3.3.1/spark-standalone.html

# Memory to allocate to the Spark master and worker daemons themselves (default: 1g).
SPARK_DAEMON_MEMORY=1g



# Spark hostoryserver
# https://spark.apache.org/docs/3.3.1/monitoring.html

# spark.history.* configuration options for the history server (default: none).
SPARK_HISTORY_OPTS="
-Dspark.history.fs.logDirectory=hdfs://hadoop-namenode:9000/spark-logs 
-Dspark.history.ui.port=18080 
-Dspark.history.fs.update.interval=10s
"

# For the filesystem history provider, the URL to the directory containing application event logs to load. 
# This can be a local file:// path, an HDFS path hdfs://namenode/shared/spark-logs or that of an alternative filesystem supported by the Hadoop APIs.
# spark.history.fs.logDirectory=file:/tmp/spark-events
# spark.history.fs.logDirectory=file:/tmp/logs/historyserve

# The port to which the web interface of the history server binds
# spark.history.ui.port=18080

# The period at which the filesystem history provider checks for new or updated logs in the log directory. 
# A shorter interval detects new applications faster, at the expense of more server load re-reading updated applications. 
# As soon as an update has completed, listings of the completed and incomplete applications will reflect the changes.
# spark.history.fs.update.interval=10s




# https://spark.apache.org/docs/3.3.1/configuration.html
# https://spark.apache.org/docs/3.3.1/monitoring.html
# https://spark.apache.org/docs/3.3.1/running-on-yarn.html
# https://spark.apache.org/docs/latest/running-on-yarn.html

# Port settings
SPARK_DEFAULTS_spark_ui_port=4040
SPARK_DEFAULTS_spark_driver_port=7001
SPARK_DEFAULTS_spark_blockManager_port=7002
SPARK_DEFAULTS_spark_port_maxRetries=10
SPARK_DEFAULTS_spark_history_ui_port=18080

# Run Spark jobs in YARN
SPARK_DEFAULTS_spark_master=yarn
# SPARK_DEFAULTS_spark_master=spark://spark-master:7077
# SPARK_DEFAULTS_spark_yarn_jars=hdfs://hadoop-namenode:9000/spark-jars/*



# Spark history server

# Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.
# spark.eventLog.enabled
SPARK_DEFAULTS_spark_eventLog_enabled=true

# Base directory in which Spark events are logged, if spark.eventLog.enabled is true. 
# Within this base directory, Spark creates a sub-directory for each application, and logs the events specific to the application in this directory. 
# Users may want to set this to a unified location like an HDFS directory so history files can be read by the history server
# spark.eventLog.dir
SPARK_DEFAULTS_spark_eventLog_dir=file:///tmp/logs/spark-events
# SPARK_DEFAULTS_spark_eventLog_dir=hdfs://hadoop-namenode:9000/spark-logs

# For the filesystem history provider, the URL to the directory containing application event logs to load. 
# This can be a local file:// path, an HDFS path hdfs://namenode/shared/spark-logs or that of an alternative filesystem supported by the Hadoop APIs.
# spark.history.fs.logDirectory
SPARK_DEFAULTS_spark_history_fs_logDirectory=file:///tmp/logs/spark-events
# SPARK_DEFAULTS_spark_history_fs_logDirectory=hdfs://hadoop-namenode:9000/spark-logs


